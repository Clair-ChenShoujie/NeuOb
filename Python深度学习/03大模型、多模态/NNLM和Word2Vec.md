# 1. N-gram
N-gram基于统计语言模型的算法，用于预测一个句子或单词序列的概率
句子的概率可以用联合概率表示：$P(w_1, w_2, \dots, w_m)$![[Pasted image 20250224184239.png|250]]

根据概率的链式法则（[[贝叶斯定理]]），联合概率可以分解为条件概率的乘积：
$P(w_1, w_2, \dots, w_m) = P(w_1) \cdot P(w_2|w_1) \cdot P(w_3|w_2, w_1) \cdot \dots \cdot P(w_m | w_{m-1}, \dots, w_1)$

但是由于上下文长度随句子增长而增加，条件概率的计算会变得非常复杂。

N-gram模型的核心思想是**马尔可夫假设（Markov Assumption）**，即：一个单词的出现只依赖于它前面的 N−1 个单词，而不是整个上下文。
$$P(w_1, w_2, \dots, w_m) \approx \prod_{i=1}^m P(w_i | w_{i-N+1}, \dots, w_{i-1})$$
这里N<<m，如果N很大接近于m就和原有链式法则相同了

 模型问题：
1. **稀疏性问题**：很多组合可能在语料库中没有出现，导致概率为 0。
    - 解决方法：平滑技术（如加法平滑、Kneser-Ney平滑）。
2. **上下文限制**：N-gram 模型的上下文长度固定，无法捕捉更长的依赖关系。
    - 解决方法：使用神经网络语言模型（如 RNN、Transformer）。

# 2. NNLM
神经网络语言模型NNLM的核心是一个多层感知机（Multi-Layer Perceptron，简称MLP）

1. 它将词向量序列**映射**到一个固定长度的向量表示（嵌入）
2. 然后将这个向量输入到一个softmax层中
3. 输出下一个词的概率分布
4. 然后就是y'-y梯度下降了

如何构建词映射矩阵
1. 先是获取大量文本数据(例如所有维基百科内容)
2. 然后我们建立一个可以沿文本滑动的窗(例如一个窗里包含三个单词)
3. 利用这样的滑动窗就能为训练模型生成大量样本数据
![[Pasted image 20250224192357.png]]
# 3. Word2Vec
为了更好的预测，其实不仅要考虑目标单词的前两个单词，还要考虑其后两个单词。

![](https://i-blog.csdnimg.cn/blog_migrate/f173ea0a8e72cf38c67c0a8a843ca1b7.png)

如果这么做，我们实际上构建并训练的模型就如下所示：

![](https://i-blog.csdnimg.cn/blog_migrate/f2533f92571c5a84b630223adc5739c2.png)

上述的这种『以上下文词汇预测当前词』架构被称为连续词袋**CBOW**

根据当前词推测当前单词可能的前后单词，这种架构就是所谓的**Skipgram**架构![](https://i-blog.csdnimg.cn/blog_migrate/cfc15193759b70ba432a67cc2d6079dc.png)
![[QQ_1740573569346.png|450]]
训练过程：
先使用随机值创建**词嵌入矩阵**和**上下文矩阵**，大小都是V\*N，V是词汇表大小，N是词向量维度
输入层的one-hot向量和**词嵌入矩阵**点乘，把one-hot向量映射为连续的稠密向量（词嵌入），得到隐藏层
隐藏层和**上下文矩阵**点乘，得到输入和上下文之间的相关性
然后用sigmoid函数把概率限制到0~1中
最后就是**error = target - sigmoid_scores**[[06线性回归|反向传播梯度下降]]，更新这两个矩阵

完成后，滑动窗口训练下一组

