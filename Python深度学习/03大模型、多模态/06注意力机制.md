​
最近在学功能脑网络，看到结合注意力机制的问题，正好整理一下。三个板块吧，自注意力机制SA，多头注意力机制MHA，交叉注意力机制CA。后续遇到其他注意力机制再添加补充。

先谈一下注意力本身，就是为了给关键特征增加权重，让模型的拟合效果更好。至于哪些特征是关键的，就需要注意力机制来计算得到**注意力系数**了。


# 1.首先是SA
这里以SNP数据为例，得到基因关联矩阵$W_B$后
![[QQ_1743475188825.png]]
输入的矩阵$W_B$可以是$N*N$的，N是样本数量，比如脑区数or节点数，这是节点关联特征。
当然，输入矩阵$W_B$大小为$N*M$，N个样本，M个特征，代表第N个样本的某个特征，也可以是一个固定样本的第T个时间片的某个特征（这里就是T*M）。

Q，K，V是矩阵$W_B$线性映射后的查询、键、值，$Q \in \mathbb{R}^{N \times d}$,$K \in \mathbb{R}^{N \times d}$,$V \in \mathbb{R}^{N \times d}$
PQ，PK，PV是三个矩阵（这里gpt的命名有点问题，应该是一个整体，右侧别当成乘Q就行）维度d为超参数，$\sqrt{d}$是缩放因子，用于避免梯度消失或爆炸，这里的两个d是同一个d。

计算得到注意力系数矩阵$I$大小$N*N$（有N个样本相互有关系）

后面再使用得到的注意力系数A来对值进行加权：
$O = I \cdot V$，维度 $N \times d$，是加权后的样本特征。
# 2.MHA多头注意力
和SA相比，多头注意力机制用了m个注意力头，分别计算每个头的注意力分布，再拼接。而SA一个注意力头，关注全局。

Q、K、V的维度本来在SA里是是$N*d$，在MHA这里为了得到不同子空间的信息，每个注意力头所计算的QKV大小为$N*\frac{d}{m}$，因为一共m个检测头。

这里拿fMRI数据的脑区关联特征矩阵$WA$为例，大小就$N*N$吧，首先对脑区关联矩阵$WA$进行线性变换，得到查询Q、键K、值V矩阵：
$Q = WA \cdot MQ_u, \quad K = WA \cdot MK_u, \quad V = WA \cdot MV_u$，这三个矩阵QKV维度都是$N*\frac{d}{m}$

m个注意力头，计算每个注意力头的，注意力系数矩阵H，$H = \text{softmax} \left( \frac{Q \cdot K^T}{\sqrt{d_k}} \right) \cdot V$，大小$N*\frac{d}{m}$

最后注意力矩阵拼接起来：
$HC = (H_1; H_2; \dots; H_m)$，得到的 HC 即为所有头部的联合注意力矩阵，大小$N*d$。

# 3.CA交叉注意力机制

在经典的交叉注意力机制中，
- **WA**（脑区特征）映射 **Q**（查询），
- **WB**（基因特征）映射 **K** 和 **V**（键和值）。

这意味着，我们通过 **WA** 生成查询矩阵 $Q_A$ ，并通过 **WB** 生成键和值矩阵 $K_B$ 和 $V_B$，然后进行注意力计算：
$\text{Attention}(Q_A, K_B, V_B) = \text{softmax}\left(\frac{Q_A K_B^T}{\sqrt{d}}\right) \cdot V_B$
A特征通过查询与B特征之间的关联来获得加权，从而聚焦于不同A和B之间的重要关系。