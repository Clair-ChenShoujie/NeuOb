单层GNN关注的点在于==Message==和==Aggregation==方式，多层关注的是层与层之间的==connectivity==，最后是关注整体的==Augmentation==。
# （1）三种GNN的基本类型
## 1.**GCN（Graph Convolutional Network）**:

**直接聚合邻居**（包括自身节点），没有明确区分自身节点和邻居节点的信息。

![[Pasted image 20241217211344.png|500]]

## 2.**GraphSAGE（Graph Sample and Aggregation）**:

首先**聚合邻居节点**的信息，然后将**自身节点**的信息与聚合结果**拼接（concat）**，再进行信息传递。

![[Pasted image 20241217211447.png|475]]
## 3.GAT 
![[Pasted image 20241224143823.png]]
## 4.案例
假设有一个节点 $v$，其邻居节点是 $\mathcal{N}(v) = { u_1, u_2, u_3 }$，且每个节点的特征是一个向量。例如，节点 $v$ 的特征是 $h_v = [1, 2]$，其邻居的特征分别是 $h_{u1} = [0.5, 1]$、$h_{u2} = [0.7, 1.3]$、$h_{u3} = [0.8, 1.2]$。
![[Pasted image 20241217212503.png]]
**GAT** ![[Pasted image 20241224143926.png]]
# （2）复杂度
![[Pasted image 20241224143014.png]]
# （3）层之间的Connectivity
GNN要经过多少跳然后获得信息，很多层反映不了模型复杂度和表现力，和CNN不一样。
## 1.**什么是感受野？**

在图神经网络（GNN）中，**感受野**表示一个节点能够通过模型间接访问到的信息范围。

- **1层GNN的感受野**：只包含节点的直接邻居（即一跳邻居）。
- **2层GNN的感受野**：包含节点的直接邻居以及邻居的邻居（二跳邻居）。
- **L层GNN的感受野**：包含节点通过 L 次消息传递可以访问到的所有节点。
## 2.**感受野是如何扩展的？**

- **假设一个节点的平均度是 d，那么在第 L 层，该节点的感受野的规模会以 $d^L$ 的速度增长。**

举个例子：
![[Pasted image 20241224153118.png]]

图的结构会显著影响感受野的扩展速度：

- **稠密图**：每个节点的邻居数量较多（节点的度很大），感受野会快速扩展。如果图特别稠密，可能很快就能覆盖全图。
- **稀疏图**：每个节点的邻居数量较少（节点的度较小），感受野扩展速度会较慢。
- **不均匀图**：在某些节点具有高度（Hub Nodes）的图中，高度节点会使得感受野扩展更快，因为它们将连接更多的远距离节点。
## 3.Oversmoothing
**Oversmoothing（过平滑）** 是指在多层GNN中，由于感受野的过度扩展和重叠，导致节点嵌入（Node Embedding）趋于相似，难以区分。

 **（1）使用shallow的GNN，同时增加GNN的表达能力**
- 增强 Message 部分
将单一线性变换替换为一个更深的多层感知机（MLP），捕获更多的非线性关系，提升表达能力。
- 增强 Aggregation 部分
将Aggregation过程从简单的“求平均”或“加权求和”，设计成一个更深的神经网络（比如用MLP对邻居特征进行组合和处理）。
 **（2）增加非传递消息层如MLP**
 **（3）跳跃连接**

# （4）图增强
几年来，特征、拓扑、结构三种类型增强的模型
![[Pasted image 20241224174405.png]]
#### 1. **图特征增强（Graph Feature Enhancement）**

##### a. **提取特征间的依赖关系（Extracting Dependencies Between Features）**

- **AM-GCN [82]**：通过k近邻图捕获节点特征之间的依赖关系。
- **CL-GNN [83]**：通过采样提取节点的局部邻域依赖关系。

##### b. **增强特征使用（Enhancing Feature Use）**

- **ACR-GNN [38]**：计算并添加全局节点特征。

---

#### 2. **图拓扑增强（Graph Topology Enhancement）**
##### a. **添加额外的拓扑信息（Adding Extra Topology Information）**

- **Twin-GNN [84]**：为节点分配颜色标签和身份。
- **ID-GNN [52]**：为节点注入唯一的颜色身份。
- **CLIP [85]**：为具有相同特征的节点子集赋予独特颜色。
- **RP-GNN [86]**：随机分配节点顺序作为其额外特征。
- **RNI-GNN [87]**：输入随机初始化的节点特征。
- **DE-GNN [88]**：捕获从给定节点到需要学习的节点的距离。

##### b. **编码微观拓扑（Encoding Micro-Topology）**

- **P-GNN [53]**：通过测量节点到随机选择的锚节点的距离进行编码。
- **PEG [89]**：使用独立的通道更新节点特征和位置特征。
- **SMP [78]**：利用独热编码学习局部拓扑信息。
- **GD-WL [90]**：编码通用化的距离信息。

##### c. **编码全局拓扑（Encoding Global-Topology）**

- **Eigen-GNN [91]**：通过对邻接矩阵进行特征值分解获取特征向量作为拓扑特征。
- **GSN [92]**：编码预定义的子图拓扑在图中的出现次数。
- **GraphSNN [93]**：基于中心节点1跳邻域子图的拓扑特征传递多样化信息。

##### d. **编码局部拓扑（Encoding Local-Topology）**

- **GNN-AK [94]**：扩展局部聚合的范围到更通用的子图。
- **NGNN [95]**：提取围绕每个节点的局部子图。
- **MPSN [96]**：在单纯复形上进行消息传递。
- **ESAN [97]**：基于预定义策略，将每个图表示为子图集合。
- **LRP-GNN [55]**：参照RP-GNN，本质上对子图进行消息传递。
- **k-GNN [25]**：高阶消息传递发生在k元子图之间。
- **k-hop GNN [98]**：消息传递发生在k跳邻域上。
- **KP-GNN [99]**：学习基于k-hop GNN的外周子图信息。

---

#### 3. **GNN架构增强（GNN Architecture Enhancement）**

##### a. **改进聚合函数（Improving Aggregation Function）**

- **GIN [27]**：通过单射函数聚合节点特征。
- **modular-GCN [100]**：使用三个GCN模块，结合不同传播策略，补偿缺失的拓扑信息。
- **diagonal-GNN [32]**：使用对角模块捕获更广泛的拓扑关系。
- **GNN-LF/HF [30]**：根据低通/高通滤波操作进行图卷积。
- **Geom-GCN [101]**：将图映射到潜在空间中，根据几何关系对邻域进行编码。
- **PG-GNN [102]**：建模邻域节点的依赖关系，确保位置不敏感。

##### b. **采用等变架构（Adopting Equivariant Architecture）**

- **k-IGN [60]**：由交替层组成，包含等变线性层和非线性激活层。
- **PPGN [103]**：在k-IGN中应用模块化策略。
- **k-FGNN [104]**：改进了k-IGN中的张量计算技术。
- **Ring-GNN [28]**：仅通过矩阵加法和矩阵乘法捕获2-IGN。
- **SUN [105]**：2-IGN的变体，同时捕获局部和全局操作。
- **GNMLL [36]**：组合多种计算操作。
- **k-MPNN [61]**：通过高阶节点关系进行消息传递计算操作。