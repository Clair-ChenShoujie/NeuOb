# 1. 前向传播
前向传播是将**输入**数据通过**神经网络**传递到**输出**的过程。在前向传播中，输入数据通过每一层神经元的处理，逐步传递到下一层，直到输出层得到最终的预测结果。

下面是一个简单的例子来说明前向传播的过程。假设我们有一个二分类问题，神经网络的输入是2个特征，输出是1个分类结果。在训练过程中，我们给定了一组输入数据（例如，[0.5, 0.6]），然后通过神经网络进行计算，得到输出结果（例如，分类为0的概率为0.8）。这个过程就是前向传播。
![|425](https://cleverbobo.github.io/2020/08/30/bp/6.png)


## 1. 前向传播计算
我们将一步步研究单隐藏层神经网络的机制，
为了简单起见，我们假设输入样本是 $\mathbf{x}\in \mathbb{R}^d$，
并且我们的隐藏层不包括偏置项。
这里的中间变量是：

$$\mathbf{z}= \mathbf{W}^{(1)} \mathbf{x},$$

其中$\mathbf{W}^{(1)} \in \mathbb{R}^{h \times d}$
是隐藏层的权重参数。
将中间变量$\mathbf{z}\in \mathbb{R}^h$通过激活函数$\phi$后，
我们得到长度为$h$的隐藏激活向量：

$$\mathbf{h}= \phi (\mathbf{z}).$$

隐藏变量$\mathbf{h}$也是一个中间变量。
假设输出层的参数只有权重$\mathbf{W}^{(2)} \in \mathbb{R}^{q \times h}$，
我们可以得到输出层变量，它是一个长度为$q$的向量：

$$\mathbf{o}= \mathbf{W}^{(2)} \mathbf{h}.$$

假设损失函数为$l$，样本标签为$y$，我们可以计算单个数据样本的损失项，

$$L = l(\mathbf{o}, y).$$

根据$L_2$正则化的定义，给定超参数$\lambda$，正则化项为

$$s = \frac{\lambda}{2} \left(\|\mathbf{W}^{(1)}\|_F^2 + \|\mathbf{W}^{(2)}\|_F^2\right),$$

其中矩阵的Frobenius范数是将矩阵展平为向量后应用的$L_2$范数。
最后，模型在给定数据样本上的正则化损失为：

$$J = L + s.$$

在下面的讨论中，我们将$J$称为**目标函数**（objective function）。

## 2.前向传播计算图

![[Pasted image 20241117122801.png]]

# 2. 反向传播
定义：通过计算损失函数对权重的梯度，来调整权重和偏差，以最小化损失函数并提高模型的预测准确性。

反向传播的过程是在每次前向传播后进行的，以便根据预测结果和真实值之间的差异来调整权重和偏差。
## 1.反向传播计算
反向传播算法存储了计算某些参数梯度时所需的任何中间变量（偏导数）。

好的，让我们详细解析反向传播（Backpropagation）的计算过程。反向传播是深度学习中用于计算梯度的重要算法，它基于链式法则，按照前向传播相反的顺序进行计算。以下是分步骤的详细解析：

### 1. 目标函数的定义
目标函数 \( J \) 包含损失项 \( L \) 和正则化项 \( s \)：
$$J = L + s$$
我们首先计算目标函数 \( J \) 相对于 \( L \) 和 \( s \) 的梯度：

$$\frac{\partial J}{\partial L} = 1 \quad \text{and} \quad \frac{\partial J}{\partial s} = 1.$$

这是因为 \( J \) 对 \( L \) 和 \( s \) 的变化率都是 1（直接相加关系）。

### 2. 计算输出层变量 $\mathbf{o}$ 的梯度
接下来，我们需要计算目标函数 \( J \) 关于输出层变量 \( \mathbf{o} \) 的梯度。根据链式法则：

$$\frac{\partial J}{\partial \mathbf{o}} = \frac{\partial J}{\partial L} \cdot \frac{\partial L}{\partial \mathbf{o}} = \frac{\partial L}{\partial \mathbf{o}} \in \mathbb{R}^q.$$

这里，\($\frac{\partial L}{\partial \mathbf{o}}$\) 是损失函数对输出层的导数，它的维度与输出层的维度 \( q \) 相同。

### 3. 计算正则化项对参数的梯度
正则化项 \( s \) 通常是参数的二范数（L2 范数）乘以系数 \( \lambda \)：

$$s = \frac{\lambda}{2} \left( \|\mathbf{W}^{(1)}\|^2 + \|\mathbf{W}^{(2)}\|^2 \right)
$$
因此，正则化项对参数的梯度为：

$$\frac{\partial s}{\partial \mathbf{W}^{(1)}} = \lambda \mathbf{W}^{(1)}, \quad \frac{\partial s}{\partial \mathbf{W}^{(2)}} = \lambda \mathbf{W}^{(2)}.$$


### 4. 计算输出层权重 \( $\mathbf{W}^{(2)}$ \) 的梯度
根据链式法则，目标函数 \( J \) 关于 \( \mathbf{W}^{(2)} \) 的梯度为：

$$\frac{\partial J}{\partial \mathbf{W}^{(2)}} = \frac{\partial J}{\partial \mathbf{o}} \cdot \frac{\partial \mathbf{o}}{\partial \mathbf{W}^{(2)}} + \frac{\partial J}{\partial s} \cdot \frac{\partial s}{\partial \mathbf{W}^{(2)}}.$$

具体来说：

$$\frac{\partial \mathbf{o}}{\partial \mathbf{W}^{(2)}} = \mathbf{h}^\top \quad \text{（假设 \( \mathbf{o} = \mathbf{W}^{(2)} \mathbf{h} + \mathbf{b}^{(2)} \)）}$$

因此：

$$\frac{\partial J}{\partial \mathbf{W}^{(2)}} = \frac{\partial J}{\partial \mathbf{o}} \cdot \mathbf{h}^\top + \lambda \mathbf{W}^{(2)}.$$

这里，\($\mathbf{h}$\) 是隐藏层的输出，维度为 \( h \)。

### 5. 计算隐藏层输出 \( $\mathbf{h}$ \) 的梯度
为了计算 \( $\mathbf{W}^{(1)}$ \) 的梯度，我们需要继续反向传播，计算目标函数 \( J \) 关于隐藏层输出 \( \mathbf{h} \) 的梯度：

$$\frac{\partial J}{\partial \mathbf{h}} = {\mathbf{W}^{(2)}}^\top \cdot \frac{\partial J}{\partial \mathbf{o}}.$$

这里，\(${\mathbf{W}^{(2)}}^\top$\) 将梯度从输出层传递到隐藏层。

### 6. 计算中间变量 \( \mathbf{z} \) 的梯度
假设隐藏层的激活函数为 \( $\phi$ \)，即 \( $\mathbf{h} = \phi(\mathbf{z}$) \)，其中 \( $\mathbf{z} = \mathbf{W}^{(1)} \mathbf{x} + \mathbf{b}^{(1)}$ \)。为了计算 \( J \) 关于 \( $\mathbf{z}$ \) 的梯度：

$$\frac{\partial J}{\partial \mathbf{z}} = \frac{\partial J}{\partial \mathbf{h}} \odot \phi'(\mathbf{z}).$$

这里，\($\odot$\) 表示按元素乘法，\($\phi'(\mathbf{z})$\) 是激活函数的导数。

### 7. 计算输入层权重 \( $\mathbf{W}^{(1)}$ \) 的梯度
最后，利用链式法则计算 \( J \) 关于 \( \mathbf{W}^{(1)} \) 的梯度：

$$\frac{\partial J}{\partial \mathbf{W}^{(1)}} = \frac{\partial J}{\partial \mathbf{z}} \cdot \mathbf{x}^\top + \lambda \mathbf{W}^{(1)}.$$

这里，\($\mathbf{x}$\) 是输入数据，维度为 \( d \)。
