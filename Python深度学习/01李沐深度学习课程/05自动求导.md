- 反向传播
![[Pasted image 20241106153117.png|288]]


# 张量的梯度
通常，我们需要传入一个与 y 形状相同的梯度，比如 y.backward(torch.ones_like(y))

### 背景知识

在 PyTorch 中，`backward()` 函数用于执行反向传播，以计算张量的梯度。对于标量（即单个数值）的输出，PyTorch 可以自动处理梯度传播，因为标量函数的梯度是一个明确的值。然而，当输出是一个非标量张量（例如向量或矩阵）时，梯度的计算变得更加复杂，因为每个输出元素都可能依赖于输入张量的多个元素。

### 标量与非标量的区别

1. **标量输出的情况**
    - **例子**：
        ```python
        y = 2 * torch.dot(x, x)  # y 是标量
        y.backward()
        ```
    - **梯度计算**：
        - 由于 `y` 是一个标量，`y.backward()` 可以自动计算 `x` 的梯度。
        - 梯度的计算基于对 `y` 关于 `x` 的全局导数，即 $\frac{\partial y}{\partial x_j}$ 对所有 $j$ 计算。

2. **非标量输出的情况**
    - **例子**：
        ```python
        y = x * x  # y 是一个向量
        y.backward(torch.ones_like(y))
        ```
    - **梯度计算**：
        - `y` 是一个与 `x` 形状相同的向量，每个元素 $y_i = x_i^2$。
        - 直接调用 `y.backward()` 会报错，因为 PyTorch 不知道如何将多维的梯度传播回 `x`。
        - 需要传入一个与 `y` 形状相同的梯度张量，通常使用 `torch.ones_like(y)`，表示对 `y` 的每个元素的梯度都是 1。

### 为什么需要传入与 `y` 形状相同的梯度？

这涉及到**链式法则**和**Jacobian 矩阵**的概念。

1. **标量输出时的梯度传播**：
    - 对于标量函数 $y = f(x)$，梯度 $\frac{\partial y}{\partial x}$ 是一个向量，表示 $y$ 对每个 $x_j$ 的偏导数。
    - PyTorch 可以自动处理这种情况，因为梯度是明确的单一向量。

2. **非标量输出时的梯度传播**：
    - 对于向量函数 $y = [y_1, y_2, ..., y_n] = f(x)$，梯度传播涉及到 Jacobian 矩阵：
        $$
        J = \begin{bmatrix}
        \frac{\partial y_1}{\partial x_1} & \dots & \frac{\partial y_1}{\partial x_m} \\
        \vdots & \ddots & \vdots \\
        \frac{\partial y_n}{\partial x_1} & \dots & \frac{\partial y_n}{\partial x_m}
        \end{bmatrix}
        $$
    - 反向传播需要计算的是向量与 Jacobian 矩阵的乘积，即 $\mathbf{v}^T J$，其中 $\mathbf{v}$ 是一个与 $y$ 形状相同的梯度向量。
    - ==如果不传入这个梯度向量，PyTorch 无法知道如何将多个输出的梯度综合起来传播回输入。==

### 为什么使用 `torch.ones_like(y)`？

传入 `torch.ones_like(y)` 的原因是：

- **作用相当于求**：
    $$
    \frac{d (\sum y_i)}{dx}
    $$  
- **在某些情况下**，你可能希望对每个输出元素都计算梯度并进行累加，这时传入全 1 的梯度是合适的。

### 具体示例

让我们通过一个具体示例来进一步理解。

```python
import torch

# 创建一个需要梯度的张量 x
x = torch.tensor([1.0, 2.0, 3.0], requires_grad=True)

# 情况一：y 是标量
y1 = 2 * torch.dot(x, x)  # y1 = 2*(1^2 + 2^2 + 3^2) = 28
y1.backward()
print("情況一 y1.backward 后的梯度:", x.grad)  # 输出: tensor([ 4.,  8., 12.])

# 清零梯度
x.grad.zero_()

# 情况二：y 是向量
y2 = x * x  # y2 = [1^2, 2^2, 3^2] = [1, 4, 9]
y2.backward(torch.ones_like(y2))  # 相当于对每个 y_i 乘以 1 再求和的梯度
print("情況二 y2.backward 后的梯度:", x.grad)  # 输出: tensor([2., 4., 6.])
```

**输出结果**：
```
情況一 y1.backward 后的梯度: tensor([ 4.,  8., 12.])
情況二 y2.backward 后的梯度: tensor([2., 4., 6.])
```

### 详细解释

1. **情況一：标量输出**

    - $y1 = 2(x_1^2 + x_2^2 + x_3^2)$
    - 对每个 $x_j$ 求导数：
        $$
        \frac{\partial y1}{\partial x_j} = 4x_j
        $$
    - 导致 `x.grad = [4., 8., 12.]`

2. **情況二：向量输出**

    - $y2 = [x_1^2, x_2^2, x_3^2]$
    - 如果我们调用 `y2.backward(torch.ones_like(y2))`，实际上是在计算：
        $$
        \sum_{i} \frac{\partial y2_i}{\partial x_j} \cdot 1 = 2x_j
        $$
    - 导致 `x.grad = [2., 4., 6.]`

![[Pasted image 20241106175844.png]]

### 总结

- **标量输出**：直接调用 `backward()`，PyTorch 自动处理梯度传播，因为梯度是单一的明确值。
- **非标量输出**：需要传入一个与输出张量形状相同的梯度张量，以明确指定每个输出元素对梯度传播的贡献。
    - 常见的做法是传入 `torch.ones_like(y)`，这相当于对所有输出元素的梯度求和。

这种机制使得 PyTorch 的反向传播更加灵活，能够处理复杂的计算图和多输出函数。理解这一点对于设计和调试深度学习模型尤为重要。