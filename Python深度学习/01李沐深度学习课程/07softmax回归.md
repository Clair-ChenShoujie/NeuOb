# 1.Logistics回归和softmax回归
[softmax和逻辑回归的区别](https://blog.csdn.net/huangfei711/article/details/79801968)

## 1. 逻辑回归的损失函数回顾

### 1.1 逻辑回归简介
逻辑回归（Logistic Regression）通常用于**二分类**问题，即预测样本属于两个类别中的哪一个。例如，判断邮件是“垃圾邮件”还是“非垃圾邮件”。

### 1.2 逻辑回归的输出
逻辑回归模型：**sigmoid**函数

公式：

$$\hat{y} = \sigma(\mathbf{w}^T \mathbf{x} + b) = \frac{1}{1 + e^{-(\mathbf{w}^T \mathbf{x} + b)}}
$$
![[Pasted image 20241115204042.png|275]]

[[09多层感知机|单层感知机]]与逻辑回归的主要不同在于激活函数与损失函数。在逻辑回归中，我们通常用sigmoid函数作为激活函数，而在单层感知机中激活函数为sign函数（阶跃函数，负0正1）。
### 1.3 逻辑回归的损失函数
逻辑回归使用**对数损失函数**（Log Loss），也称为**二元交叉熵损失**。

对于单个样本，损失函数定义为：

$$l(y, \hat{y}) = - [y \log(\hat{y}) + (1 - y) \log(1 - \hat{y})]
$$

## 2. Softmax 回归（多分类逻辑回归）

### 2.1 Softmax 函数
为了处理多分类问题，我们需要将模型的输出转换为每个类别的概率分布，这时引入了**Softmax**函数。
![[Pasted image 20241115201041.png]]![[Pasted image 20241115201004.png]]


#### 2.1.1 Softmax 的定义
对于 \( q \) 个类别，Softmax 函数将一个长度为 \( q \) 的向量 $\mathbf{o} = [o_1, o_2, ..., o_q]$ 转换为一个概率分布 $\hat{\mathbf{y}} = [\hat{y}_1, \hat{y}_2, ..., \hat{y}_q]$，
其中：
$$
\hat{y}_j = \frac{e^{o_j}}{\sum_{k=1}^{q} e^{o_k}} \quad \text{对所有 } j = 1, 2, ..., q
$$
**特点**：
- 每个  $\hat{y}_j$  都在 \( [0, 1] \) 之间。
- 所有  $\hat{y}_j$  之和为1，形成一个有效的概率分布。

### 2.2 Softmax 回归的损失函数

#### 2.2.1 交叉熵损失
Softmax 回归使用**交叉熵损失**（Cross-Entropy Loss）来衡量预测分布与真实分布之间的差异。

对于单个样本，假设真实标签为独热编码向量 \( $\mathbf{y} = [y_1, y_2, ..., y_q]$ \)，损失函数定义为：

$$l(\mathbf{y}, \hat{\mathbf{y}}) = - \sum_{j=1}^{q} y_j \log(\hat{y}_j)$$

由于 \( \mathbf{y} \) 是独热编码，只有一个 \( y_j = 1 \)，其余为0。因此，实际计算时仅涉及真实类别对应的预测概率。

#### 2.2.2 损失函数的几何意义
交叉熵损失衡量了两个概率分布之间的差异：
- **熵**（Entropy）：真实分布 \( P \) 的不确定性。
- **交叉熵**（Cross-Entropy）：模型分布 \( Q \) 对 \( P \) 的不确定性。

目标是最小化交叉熵，使模型分布 \( Q \) 尽可能接近真实分布 \( P \)。

### 2.3 Softmax 与逻辑回归的对比

| 特性                | 逻辑回归（二分类）                     | Softmax 回归（多分类）                   |
|---------------------|--------------------------------------|----------------------------------------|
| **适用问题**         | 二分类问题                             | 多分类问题（3个及以上类别）             |
| **输出函数**         | Sigmoid（将输出映射到 [0,1] 间的概率） | Softmax（将输出映射为概率分布）          |
| **损失函数**         | 对数损失（Binary Cross-Entropy）      | 交叉熵损失（Cross-Entropy Loss）      |
| **模型架构**         | 单一输出节点                           | 多个输出节点，每个节点对应一个类别      |
| **梯度计算**         | 基于预测概率与真实标签的差异            | 基于各类别预测概率与真实标签的差异        |

**相似点**：
- 两者都是线性模型，通过学习权重和偏置来进行分类。
- 均使用最大似然估计（Maximum Likelihood Estimation）来优化模型参数。

**不同点**：
- 逻辑回归处理二分类，Softmax 回归扩展到多分类。
- Softmax 函数能够同时输出多个类别的概率，而逻辑回归仅输出一个二分类的概率。

## 3. 损失函数的优化与梯度

### 3.1 损失函数的优化目标
目标是最小化整个数据集上的损失函数，通常使用**梯度下降**或其变种进行优化。

### 3.2 梯度计算
对于 Softmax 回归，损失函数关于模型输出 \( o_j \) 的梯度为：

$$\frac{\partial l}{\partial o_j} = \hat{y}_j - y_j$$

**解释**：
- \( $\hat{y}_j$ \) 是模型预测的类别 \( j \) 的概率。
- \( $y_j$ \) 是真实标签的独热编码值（0 或 1）。

**含义**：
- 当预测概率高于真实标签时，梯度为正，促使权重调整以降低 \( \hat{y}_j \)。
- 当预测概率低于真实标签时，梯度为负，促使权重调整以增加 \( \hat{y}_j \)。

### 3.3 参数更新
使用梯度下降法更新权重和偏置：
$$
\mathbf{W} := \mathbf{W} - \eta \frac{\partial L}{\partial \mathbf{W}}
$$

$$
\mathbf{b} := \mathbf{b} - \eta \frac{\partial L}{\partial \mathbf{b}}
$$
其中，\( $\eta$ \) 是学习率，\($L$ \) 是总损失。

# 2. 极大似然估计（Maximum Likelihood Estimation, MLE）的直观理解

**极大似然估计**是一种用于估计统计模型参数的方法。其基本思想是：

- **事件和概率**：假设我们有一个观测到的数据集，这些数据可以看作是某个概率分布下的事件。
- **联合概率**：如果这些数据是独立同分布的（Independent and Identically Distributed, IID），整个数据集发生的概率就是各个数据点概率的乘积。
  
  例如，假设我们有三个观测值 \( x_1, x_2, x_3 \)，对应的概率分别为 \( p_1, p_2, p_3 \)，那么整个事件发生的联合概率就是 \( $p_1 \times p_2 \times p_3$ \)。

- **极大化联合概率**：为了找到最可能生成这些数据的参数，我们需要最大化这个联合概率。也就是说，我们要找到使得 \( $p_1 \times p_2 \times p_3$ \) 最大化的参数值。

## 1. 似然函数与Softmax

**==似然函数==** \( $L(\theta)$ \) 描述了参数 \( $\theta$ \) 下，观测数据出现的概率：

$$L(\theta) = P(\text{数据} | \theta)$$

在分类问题中，模型的输出通常通过 **Softmax 函数** 转化为概率分布。假设我们有一个包含 \( C \) 个类别的分类问题，模型的输出 \( $\mathbf{z} = [z_1, z_2, \ldots, z_C]$ \) 通过 Softmax 变换后得到各类别的预测概率：

$$\text{Softmax}(z_i) = \frac{e^{z_i}}{\sum_{j=1}^{C} e^{z_j}}$$

对于每个样本，如果目标类别是 \( y \)，那么似然函数中该样本的概率就是对应类别的 Softmax 输出 \( p_y \)。

**对于整个数据集**（假设有 \( N \) 个样本），似然函数就是所有样本对应 Softmax 输出的乘积：

$$L(\theta) = \prod_{i=1}^{N} p_{y_i}$$

这就是你提到的“将所有输出的 Softmax 相乘”。

## 2. 从似然函数到交叉熵损失

直接最大化似然函数在计算上不方便，因此通常对似然函数取对数，得到对数似然函数：

$$\log L(\theta) = \sum_{i=1}^{N} \log p_{y_i}$$

**优化目标**通常是最大化对数似然，即：

$$\max_{\theta} \log L(\theta) = \max_{\theta} \sum_{i=1}^{N} \log p_{y_i}$$

为了便于优化，尤其是使用梯度下降法等方法，常将最大化问题转换为最小化问题，即最小化负对数似然：


$$\min_{\theta} -\sum_{i=1}^{N} \log p_{y_i}$$

这个负对数似然损失函数实际上就是 **交叉熵损失函数**（Cross-Entropy Loss）：

$$\text{Cross-Entropy} = -\sum_{i=1}^{N} \sum_{c=1}^{C} y_{i,c} \log p_{i,c}$$

其中 \( $y_{i,c}$ \) 是样本 \( i \) 的真实标签，通常是一个 one-hot 向量，只有正确类别的位置为1，其余为0。

## 3. 梯度优化与参数估计

为了找到参数 \( \theta \) 的最优值，使得交叉熵损失最小，我们需要对损失函数关于 \( \theta \) 求导数，并找到导数为零的点。这涉及到以下步骤：

1. **求导**：计算损失函数对每个参数的偏导数。
2. **梯度下降**：利用这些导数信息，采用梯度下降或其他优化算法迭代更新参数，逐步逼近最优值。
3. **收敛**：当参数更新趋于稳定，损失函数达到最小值时，得到的参数即为极大似然估计的结果。

### 总结

- **似然函数**在分类问题中，等同于将所有样本的 Softmax 输出概率相乘。
- **最大化似然函数**相当于最小化负对数似然，即 **交叉熵损失函数**。
- 通过优化交叉熵损失，可以估计出模型参数，使得模型对训练数据的解释概率最大化。

# 代码实现
- 初始化方法
```
net = nn.Sequential(nn.Flatten(), nn.Linear(784, 10))

def init_weights(m):
    if type(m) == nn.Linear:
        nn.init.normal_(m.weight, std=0.01)

net.apply(init_weights);
```

