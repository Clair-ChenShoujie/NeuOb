梯度爆炸（gradient exploding）问题： 参数更新过大，破坏了模型的稳定收敛。
梯度消失（gradient vanishing）问题： 参数更新过小，在每次更新时几乎不会移动，导致模型无法学习。

## 一、梯度基础
### 1、什么是梯度

梯度是一个与函数相切的向量，指向此函数==最大增量==的方向。 函数在局部==最大值==或==最小值==处梯度为零。

在数学中，梯度被定义为函数的==偏导数==。

### 2、什么是损失函数

神经网络损失函数神经网络的损失函数是衡量模型**预测结果**与**真实结果**之间差距的指标

### 3、梯度下降

由于梯度是指向函数最大增量的向量，因此负梯度是指向函数最大减量的向量。 因此，梯度下降就是通过在负梯度方向上迭代来最小化损失函数。

神经网络推理过程中==梯度信息重复==导致计算量过高。

但同时通过**截断梯度流**防止了过多的重复梯度信息。该思想通过设计一种分层的特征融合策略来实现

### 4、SPPFCSPC
SPPFCSPC==梯度流控制：==
https://www.cnblogs.com/armcvai/p/16793177.html

- 直接跳过的部分获得无重复的梯度
- 没有完全消除梯度重复，但SPPF引起的梯度重复被限制在部分特征中，显著减少了重复的程度和影响范围。

为什么SPPFCSPC更有效

- 梯度分流：部分特征完全绕过SPPF，避免了梯度重复
- 局部化重复：SPP引起的梯度重复被限制在部分特征中
- 信息平衡：保留了原始特征信息，同时获得多尺度表示
- 计算效率：减少了需要经过SPPF的特征量，降低了计算成本

## 二、特征缩放
**特征缩放**（Feature Scaling）是数据预处理中的一种方法，旨在将数据的特征值缩放到一个相对较小的范围内。其核心目的是消除不同特征之间由于量纲（单位）或数值范围差异导致的影响，从而提高机器学习算法的性能和训练速度。

---

### **一、为什么需要特征缩放**

1. **消除量纲差异**：在现实数据中，不同特征的取值范围可能相差很大。例如，房屋的面积可能以平方米计，数值上达到数百，而房间数量可能只有个位数。如果直接使用这些数据进行训练，数值较大的特征会对模型产生更大的影响。

2. **提高优化算法效率**：对于使用梯度下降等优化算法的模型，特征缩放可以使代价函数的等高线更加圆形，避免曲面过于扁平，导致收敛缓慢。

3. **加速模型训练**：统一特征的尺度可以加快模型的训练速度，减少迭代次数。

4. **提高模型精度**：某些距离度量敏感的算法（如K近邻、K均值聚类）对特征的尺度非常敏感，特征缩放可以提高这些算法的性能。

---

### **二、常见的特征缩放方法**

==1. **标准化（Standardization）**==

   - **公式**：

$x^{\prime} = \frac{x - \mu}{\sigma}$
  
   其中，\( \mu \) 为特征的均值，\( \sigma \) 为特征的标准差。

   - **特点**：将特征转换为均值为0，标准差为1的标准正态分布。

   - **适用场景**：当数据存在异常值，且希望保留异常值的影响时。

==2. **归一化（Normalization）**==

   - **公式**：

$x^{\prime} = \frac{x - x_{\min}}{x_{\max} - x_{\min}}$
  
     其中，\( x_{\min} \) 和 \( x_{\max} \) 分别为特征的最小值和最大值。

   - **特点**：将特征值缩放到 [0, 1] 的范围内。

   - **适用场景**：当需要将特征值限制在特定范围，或数据分布没有明显的集中趋势时。

==3. **最大绝对值缩放**==

   - **公式**：

$x^{\prime} = \frac{x}{\max{|x|}}$
  
   - **特点**：将特征值缩放到 [-1, 1] 的范围。

   - **适用场景**：当数据集中含有零中心的稀疏数据时。

==4. **对数变换**==

   - **公式**：

$x^{\prime} = \log(x + c)$


     其中，\( c \) 为常数，用于避免对零取对数。

   - **特点**：压缩较大的数值范围，使数据分布更对称。

   - **适用场景**：处理具有偏态分布的特征，如收入、人口等。

---

### **三、特征缩放在机器学习中的应用**

1. **优化算法的收敛速度**

   - **梯度下降**：特征缩放后，代价函数的等高线更接近于同心圆，梯度下降路径更加直接，收敛速度加快。

2. **距离度量算法**

   - **K近邻（KNN）**：计算样本之间的欧氏距离，未经缩放的特征会导致某些维度主导距离计算。

   - **支持向量机（SVM）**：使用核函数时，特征的尺度会影响到超平面的构建。

3. **正则化模型**

   - **[[2023-07-12回归分析|岭回归、Lasso回归]]**：正则化项中包含参数的平方和绝对值，特征缩放可以平衡不同特征对正则化的影响。

---

### **四、示例**

假设有如下数据集：

| 样本编号 | 身高（cm） | 体重（kg） |
| -------- | --------- | --------- |
| 1        | 170       | 65        |
| 2        | 160       | 70        |
| 3        | 180       | 80        |

**标准化处理**：

1. 计算均值和标准差：

   - 身高均值：$( \mu_{height} = \frac{170 + 160 + 180}{3} = 170 )$
   - 身高标准差：$( \sigma_{height} = \sqrt{\frac{(170-170)^2 + (160-170)^2 + (180-170)^2}{3}} \approx 8.16 )$
   - 体重均值：$( \mu_{weight} = \frac{65 + 70 + 80}{3} \approx 71.67 )$
   - 体重标准差：$( \sigma_{weight} = \sqrt{\frac{(65-71.67)^2 + (70-71.67)^2 + (80-71.67)^2}{3}} \approx 7.64 )$

2. 进行标准化：

   - 样本1的身高标准化值：

$$     
     z = \frac{170 - 170}{8.16} = 0
     $$

   - 样本1的体重标准化值：


$$     z = \frac{65 - 71.67}{7.64} \approx -0.87$$
  
